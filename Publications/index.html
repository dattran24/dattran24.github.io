<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Dat Tran </title> <meta name="author" content="Dat Tran"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dattran24.github.io/Publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dat</span> Tran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/Publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/Teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks </a> </li> <li class="nav-item "> <a class="nav-link" href="/CV/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Search"></p> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style=""> <div style="color: white;">Conference</div> </abbr> </div> <div id="klmtneurips" class="col-sm-8"> <div class="title">Fundamental Convergence Analysis of Sharpness-Aware Minimization</div> <div class="author"> P. D. Khanh, H-C Luong, B. S. Mordukhovich, and <em>D. B. Tran</em> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/SAMNeurIPS24.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://nips.cc/virtual/2024/poster/95276" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="badges"> <span class="altmetric-embed" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=9V5qSRcAAAAJ&amp;citation_for_view=9V5qSRcAAAAJ:9V5qSRcAAAAJ:u-x6o8ySG0sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The paper investigates the fundamental convergence properties of Sharpness-Aware Minimization (SAM), a recently proposed gradient-based optimization method (Foret et al., 2021) that significantly improves the generalization of deep neural networks. The convergence properties including the stationarity of accumulation points, the convergence of the sequence of gradients to the origin, the sequence of function values to the optimal value, and the sequence of iterates to the optimal solution are established for the method. The universality of the provided convergence analysis based on inexact gradient descent frameworks (Khanh et al., 2023b) allows its extensions to the normalized versions of SAM such as VaSSO (Li &amp; Giannakis, 2023), RSAM (Liu et al., 2022), and to the unnormalized versions of SAM such as USAM (Andriushchenko &amp; Flammarion, 2022). Numerical experiments are conducted on classification tasks using deep learning models to confirm the practical aspects of our analysis.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">klmtneurips</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fundamental Convergence Analysis of Sharpness-Aware Minimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khanh, P. D. and Luong, H-C and Mordukhovich, B. S. and Tran, D. B.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://papers.nips.cc/paper_files/paper/2024/hash/17b08a9de93e2accf13429643e7eafdc-Abstract-Conference.html}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color: #4c027d; color: white !important;"> <div style="color: white;">Journal</div> </abbr> </div> <div id="kmptMAPR" class="col-sm-8"> <div class="title">Globally convergent coderivative-based generalized Newton methods in nonsmooth optimization</div> <div class="author"> P. D. Khanh, B. S. Mordukhovich, V. T Phat, and <em>D. B. Tran</em> </div> <div class="periodical"> <em>Mathematical Programming</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s10107-023-01980-2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Newton_MAPR.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/s10107-023-01980-2" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=9V5qSRcAAAAJ&amp;citation_for_view=9V5qSRcAAAAJ:u-x6o8ySG0sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper proposes and justifies two globally convergent Newton-type methods to solve unconstrained and constrained problems of nonsmooth optimization by using tools of variational analysis and generalized differentiation. Both methods are coderivative-based and employ generalized Hessians (coderivatives of subgradient mappings) associated with objective functions, which are either of class C1,1, or are represented in the form of convex composite optimization, where one of the terms may be extended-real-valued. The proposed globally convergent algorithms are of two types. The first one extends the damped Newton method and requires positive-definiteness of the generalized Hessians for its well-posedness and efficient performance, while the other algorithm is of the regularized Newton type being well-defined when the generalized Hessians are merely positive-semidefinite. The obtained convergence rates for both methods are at least linear, but become superlinear under the semismooth∗ property of subgradient mappings. Problems of convex composite optimization are investigated with and without the strong convexity assumption on smooth parts of objective functions by implementing the machinery of forward-backward envelopes. Numerical experiments are conducted for Lasso problems and for box constrained quadratic programs with providing performance comparisons of the new algorithms and some other first-order and second-order methods that are highly recognized in nonsmooth optimization.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kmptMAPR</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Globally convergent coderivative-based generalized Newton methods in nonsmooth optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khanh, P. D. and Mordukhovich, B. S. and Phat, V. T and Tran, D. B.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Mathematical Programming}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{205}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{373-429}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s10107-023-01980-2}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color: #4c027d; color: white !important;"> <div style="color: white;">Journal</div> </abbr> </div> <div id="kmtOMS" class="col-sm-8"> <div class="title">A new inexact gradient descent method with applications to nonsmooth convex optimization</div> <div class="author"> P. D. Khanh, B. S. Mordukhovich, and <em>D. B. Tran</em> </div> <div class="periodical"> <em>Optimization Methods and Software</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.tandfonline.com/doi/full/10.1080/10556788.2024.2322700" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/OMS.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1080/10556788.2024.2322700" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=9V5qSRcAAAAJ&amp;citation_for_view=9V5qSRcAAAAJ:IjCSPb-OGe4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The paper proposes and develops a novel inexact gradient method (IGD) for minimizing C1-smooth functions with Lipschitzian gradients, i.e., for problems of C1,1 optimization. We show that the sequence of gradients generated by IGD converges to zero. The convergence of iterates to stationary points is guaranteed under the Kurdyka- Lojasiewicz (KL) property of the objective function with convergence rates depending on the KL exponent. The newly developed IGD is applied to designing two novel gradient-based methods of nonsmooth convex optimization such as the inexact proximal point methods (GIPPM) and the inexact augmented Lagrangian method (GIALM) for convex programs with linear equality constraints. These two methods inherit global convergence properties from IGD and are confirmed by numerical experiments to have practical advantages over some well-known algorithms of nonsmooth convex optimization.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kmtOMS</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A new inexact gradient descent method with applications to nonsmooth convex optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khanh, P. D. and Mordukhovich, B. S. and Tran, D. B.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Optimization Methods and Software}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Taylor and Francis}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-29}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1080/10556788.2024.2322700}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color: #4c027d; color: white !important;"> <div style="color: white;">Journal</div> </abbr> </div> <div id="kmptJOGOprox" class="col-sm-8"> <div class="title">Inexact proximal methods for weakly convex functions</div> <div class="author"> P. D. Khanh, B. S. Mordukhovich, V. T Phat, and <em>D. B. Tran</em> </div> <div class="periodical"> <em>to appear Journal of Global Optimization</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2307.15596" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/proximal.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=9V5qSRcAAAAJ&amp;citation_for_view=9V5qSRcAAAAJ:zYLM7Y9cAGgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper proposes and develops inexact proximal methods for finding stationary points of the sum of a smooth function and a nonsmooth weakly convex one, where an error is present in the calculation of the proximal mapping of the nonsmooth term. A general framework for finding zeros of a continuous mapping is derived from our previous paper on this subject to establish convergence properties of the inexact proximal point method when the smooth term is vanished and of the inexact proximal gradient method when the smooth term satisfies a descent condition. The inexact proximal point method achieves global convergence with constructive convergence rates when the Moreau envelope of the objective function satisfies the Kurdyka-Lojasiewicz (KL) property. Meanwhile, when the smooth term is twice continuously differentiable with a Lipschitz continuous gradient and a differentiable approximation of the objective function satisfies the KL property, the inexact proximal gradient method achieves the global convergence of iterates with constructive convergence rates.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kmptJOGOprox</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Inexact proximal methods for weakly convex functions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khanh, P. D. and Mordukhovich, B. S. and Phat, V. T and Tran, D. B.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{to appear Journal of Global Optimization}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color: gray; color: white !important;"> <div style="color: white;">Preprint</div> </abbr> </div> <div id="kmtMAPR" class="col-sm-8"> <div class="title">Globally Convergent Derivative-Free Methods in Nonconvex Optimization with and without Noise</div> <div class="author"> P. D. Khanh, B. S. Mordukhovich, and <em>D. B. Tran</em> </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://optimization-online.org/?p=26889" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Derivative_free_24_07_01.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/dattran24/Derivative-free" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=9V5qSRcAAAAJ&amp;citation_for_view=9V5qSRcAAAAJ:eQOLeE2rZwMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper addresses the study of nonconvex derivative-free optimization problems, where only information of either smooth objective functions or their noisy approximations is available. General derivative-free methods are proposed for minimizing differentiable (not necessarily convex) functions with globally Lipschitz continuous gradients, where the accuracy of approximate gradients is interacting with stepsizes and exact gradient values. Analysis in the noiseless case guarantees convergence of the gradient sequence to the origin as well as global convergence with constructive convergence rates of the sequence of iterates under the Kurdyka-Łojasiewicz property. In the noisy case, without any noise level information, the designed algorithms reach near-stationary points with providing estimates on the required number of iterations and function evaluations. Addressing functions with locally Lipschitzian gradients, two algorithms are introduced to handle the noiseless and noisy cases, respectively. The noiseless version is based on the standard backtracking linesearch and achieves fundamental convergence properties similarly to the global Lipschitzian case. The noisy version is based on a novel bidirectional linesearch and is shown to reach near-stationary points after a finite number of iterations when the Polyak-Łojasiewicz inequality is imposed. Numerical experiments are conducted on a diverse set of test problems to demonstrate more robustness of the newly proposed algorithms in comparison with other finite-difference-based schemes and some highly efficient, production-ready codes from the SciPy library.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kmtMAPR</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Globally Convergent Derivative-Free Methods in Nonconvex Optimization with and without Noise}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khanh, P. D. and Mordukhovich, B. S. and Tran, D. B.}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://optimization-online.org/?p=26889}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color: gray; color: white !important;"> <div style="color: white;">Preprint</div> </abbr> </div> <div id="ckmtOptim" class="col-sm-8"> <div class="title">Local Convergence Analysis for Nonisolated Solutions to Derivative-Free Methods of Optimization</div> <div class="author"> D. H. Cuong, P. D. Khanh, B. S. Mordukhovich, and <em>D. B. Tran</em> </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://optimization-online.org/?p=27216" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Local_derivative_free.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=9V5qSRcAAAAJ&amp;citation_for_view=9V5qSRcAAAAJ:WF5omc3nYNoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Local Convergence Analysis for Nonisolated Solutions to Derivative-Free Methods of Optimization</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ckmtOptim</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Local Convergence Analysis for Nonisolated Solutions to Derivative-Free Methods of Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cuong, D. H. and Khanh, P. D. and Mordukhovich, B. S. and Tran, D. B.}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://optimization-online.org/?p=27216}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style=""> <div style="color: white;">Conference</div> </abbr> </div> <div id="klmttOptim" class="col-sm-8"> <div class="title">Convergence of Sharpness-Aware Minimization with Momentum</div> <div class="author"> P. D. Khanh, H.-C. Luong, B. S. Mordukhovich, <em>D. B. Tran</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'T. Vo' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-73420-5_11" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=9V5qSRcAAAAJ&amp;citation_for_view=9V5qSRcAAAAJ:9V5qSRcAAAAJ" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper presents an analysis of Sharpness-Aware Minimization (SAM), a recently introduced efficient optimizer that has demonstrated remarkable improvements in the generalization of deep neural networks. A comprehensive analysis of the asymptotic convergence behaviors of the optimizer when integrated with momentum is proposed. We first show the convergence of the gradient sequence to zero and the topological properties of the set of accumulation points generated by the iterative sequence. Under the assumption of the isolation of stationary points, especially when the function is strongly convex, the convergence of the sequence of iterates is ensured. To validate the practical implications of our analysis, we conduct numerical experiments on classification tasks employing well-known deep learning models, including ResNet18 and ResNet34, with standard datasets CIFAR-10, CIFAR-100, MNIST, and Fashion-MNIST. The numerical results show that, in general, incorporating momentum improves both the training process and testing accuracy for SAM rather than just using standard SGD.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">klmttOptim</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Convergence of Sharpness-Aware Minimization with Momentum}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khanh, P. D. and Luong, H.-C. and Mordukhovich, B. S. and Tran, D. B. and Vo, T.}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://link.springer.com/chapter/10.1007/978-3-031-73420-5_11}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color: #4c027d; color: white !important;"> <div style="color: white;">Journal</div> </abbr> </div> <div id="kmptJOGO" class="col-sm-8"> <div class="title">Generalized damped Newton algorithms in nonsmooth optimization via second-order subdifferentials</div> <div class="author"> P. D. Khanh, B. S. Mordukhovich, V. T Phat, and <em>D. B. Tran</em> </div> <div class="periodical"> <em>Journal of Global Optimization</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s10898-022-01248-7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/NewtonJOGO.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/dattran24/GDNM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/s10898-022-01248-7" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=9V5qSRcAAAAJ&amp;citation_for_view=9V5qSRcAAAAJ:d1gkVwhDpl0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The paper proposes and develops new globally convergent algorithms of the generalized damped Newton type for solving important classes of nonsmooth optimization problems. These algorithms are based on the theory and calculations of second-order subdifferentials of nonsmooth functions with employing the machinery of second-order variational analysis and generalized differentiation. First we develop a globally superlinearly convergent damped Newton-type algorithm for the class of continuously differentiable functions with Lipschitzian gradients, which are nonsmooth of second order. Then we design such a globally convergent algorithm to solve a structured class of nonsmooth quadratic composite problems with extended-real-valued cost functions, which typically arise in machine learning and statistics. Finally, we present the results of numerical experiments and compare the performance of our main algorithm applied to an important class of Lasso problems with those achieved by other first-order and second-order optimization algorithms.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kmptJOGO</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generalized damped Newton algorithms in nonsmooth optimization via second-order subdifferentials}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khanh, P. D. and Mordukhovich, B. S. and Phat, V. T and Tran, D. B.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Global Optimization}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{86}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{93-122}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s10898-022-01248-7}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color: #4c027d; color: white !important;"> <div style="color: white;">Journal</div> </abbr> </div> <div id="kmtJOTA" class="col-sm-8"> <div class="title">Inexact reduced gradient methods in nonconvex optimization</div> <div class="author"> P. D. Khanh, B. S. Mordukhovich, and <em>D. B. Tran</em> </div> <div class="periodical"> <em>Journal of Optimization Theory and Applications</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s10957-023-02319-9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/IRG.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/s10957-023-02319-9" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=9V5qSRcAAAAJ&amp;citation_for_view=9V5qSRcAAAAJ:UeHWp8X0CEIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper proposes and develops new linesearch methods with inexact gradient information for finding stationary points of nonconvex continuously differentiable functions on finite-dimensional spaces. Some abstract convergence results for a broad class of linesearch methods are stablished. A general scheme for inexact reduced gradient (IRG) methods is proposed, where the errors in the gradient approximation automatically adapt with the magnitudes of the exact gradients. The sequences of iterations are shown to obtain stationary accumulation points when different stepsize selections are employed. Convergence results with constructive convergence rates for the developed IRG methods are established under the Kurdyka- Lojasiewicz property. The obtained results for the IRG methods are confirmed by encouraging numerical experiments, which demonstrate advantages of automatically controlled errors in IRG methods over other frequently used error selections.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kmtJOTA</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Inexact reduced gradient methods in nonconvex optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khanh, P. D. and Mordukhovich, B. S. and Tran, D. B.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Optimization Theory and Applications}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-41}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s10957-023-02319-9}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dat Tran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/Publications/"}},{id:"nav-teaching",title:"Teaching",description:"",section:"Navigation",handler:()=>{window.location.href="/Teaching/"}},{id:"nav-talks",title:"Talks",description:"",section:"Navigation",handler:()=>{window.location.href="/talks/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/CV/"}},{id:"nav-people",title:"People",description:"",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-our-paper-fundamental-convergence-analysis-of-sharpness-aware-minimization-has-been-accepted-in-advances-in-neural-information-processing-systems-neurips-2024-sparkles-smile",title:"Our paper, Fundamental Convergence Analysis of Sharpness-Aware Minimization, has been accepted in Advances...",description:"",section:"News"},{id:"news-i-will-be-joining-rowan-university-as-an-assistant-professor-in-mathematics-beginning-september-1-2025-i-am-also-looking-for-self-motivated-ph-d-students-with-an-interest-in-optimization-data-science",title:"I will be joining Rowan University as an Assistant Professor in Mathematics, beginning...",description:"",section:"News"},{id:"projects-calculus-i",title:"Calculus I",description:"Introduction to the foundational concepts of calculus, focusing on limits, continuity, derivatives, and their applications",section:"Projects",handler:()=>{window.location.href="/projects/1_calculus/"}},{id:"projects-elementary-functions",title:"Elementary Functions",description:"This course serves as a bridge between algebra and calculus, focusing on the study of essential functions and their properties.",section:"Projects",handler:()=>{window.location.href="/projects/2_elementary/"}},{id:"projects-college-algebra",title:"College Algebra",description:"This course is designed to provide a solid foundation in algebraic principles, focusing on the understanding and manipulation of algebraic expressions, equations, and functions.",section:"Projects",handler:()=>{window.location.href="/projects/3_algebra/"}},{id:"projects-multiobjective-optimization",title:"Multiobjective Optimization",description:"This course focuses on the theory and methods for optimizing problems with multiple, often conflicting, objectives. Unlike single-objective optimization, where the goal is to find a single optimal solution, multi-objective optimization seeks to balance trade-offs between competing objectives",section:"Projects",handler:()=>{window.location.href="/projects/4_multi/"}},{id:"projects-convex-analysis",title:"Convex Analysis",description:"This course explores the theory of convex sets and convex functions, which play a crucial role in optimization, economics, and various branches of mathematics. The course covers the fundamental concepts of convexity, providing students with the tools to analyze and solve convex optimization problems",section:"Projects",handler:()=>{window.location.href="/projects/5_convex/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%74%72%61%6E%62%61%64%61%74@%77%61%79%6E%65.%65%64%75","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0001-6711-7471","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=9V5qSRcAAAAJ","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Dat-Tran-39/","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/dat-tran-4b778a202","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>