---
---

@string{aps = {American Physical Society,}}


@article{klmtneurips,
  abbr={Conference},
  title={Fundamental Convergence Analysis of Sharpness-Aware Minimization},
  author={Khanh, P. D. and Luong, H-C and Mordukhovich, B. S. and Tran, D. B.},
  abstract={The paper investigates the fundamental convergence properties of Sharpness-Aware Minimization (SAM), a recently proposed gradient-based optimization method (Foret et al., 2021) that significantly improves the generalization of deep neural networks. The convergence properties including the stationarity of accumulation points, the convergence of the sequence of gradients to the origin, the sequence of function values to the optimal value, and the sequence of iterates to the optimal solution are established for the method. The universality of the provided convergence analysis based on inexact gradient descent frameworks (Khanh et al., 2023b) allows its extensions to the normalized versions of SAM such as VaSSO (Li & Giannakis, 2023), RSAM (Liu et al., 2022), and to the unnormalized versions of SAM such as USAM (Andriushchenko & Flammarion, 2022). Numerical experiments are conducted on classification tasks using deep learning models to confirm the practical aspects of our analysis.},
  journal={to appear Advances in Neural Information Processing Systems},
  year={2024},
  month={September},
  publisher=aps,
  url={https://arxiv.org/abs/2401.08060},
  html={https://neurips.cc/},
  pdf={SAM_Neurips.pdf},
  altmetric={},
  dimensions={true},
  google_scholar_id={W7OEmFMy1HYC},
  selected={true},
}
@article{kmptMAPR,
  abbr={Journal},
  title={Globally convergent coderivative-based generalized Newton methods in nonsmooth optimization},
  abstract={This paper proposes and justifies two globally convergent Newton-type methods to solve unconstrained and constrained problems of nonsmooth optimization by using tools of variational analysis and generalized differentiation. Both methods are coderivative-based and employ generalized Hessians (coderivatives of subgradient mappings) associated with objective functions, which are either of class C1,1, or are represented in the form of convex composite optimization, where one of the terms may be extended-real-valued. The proposed globally convergent algorithms are of two types. The first one extends the damped Newton method and requires positive-definiteness of the generalized Hessians for its well-posedness and efficient performance, while the other algorithm is of {the regularized Newton type} being well-defined when the generalized Hessians are merely positive-semidefinite. The obtained convergence rates for both methods are at least linear, but become superlinear under the semismoothâˆ— property of subgradient mappings. Problems of convex composite optimization are investigated with and without the strong convexity assumption {on smooth parts} of objective functions by implementing the machinery of forward-backward envelopes. Numerical experiments are conducted for Lasso problems and for box constrained quadratic programs with providing performance comparisons of the new algorithms and some other first-order and second-order methods that are highly recognized in nonsmooth optimization.},
  author={Khanh, P. D. and Mordukhovich, B. S. and Phat, V. T and Tran, D. B.},
  journal={Mathematical Programming},
  volume={205},
  pages={373-429},
  year={2024},
  doi={10.1007/s10107-023-01980-2},
  publisher=Springer,
  html={https://link.springer.com/article/10.1007/s10107-023-01980-2},
  dimensions={true},
  pdf={Newton_MAPR.pdf},
  google_scholar_id={u-x6o8ySG0sC},
  selected={true},
}

@article{kmptJOGO,
  abbr={Journal},
  title={Generalized damped Newton algorithms in nonsmooth optimization via second-order subdifferentials},
  author={PD Khanh, BS Mordukhovich, VT Phat, DB Tran},
  journal={Journal of Global Optimization},
  volume={86},
  pages={93-122},
  year={2023}
}

@article{kmtJOTA,
  abbr={Journal},
  title={Inexact reduced gradient methods in nonconvex optimization},
  abstract={This paper proposes and develops new linesearch methods with inexact gradient information for finding stationary points of nonconvex continuously differentiable functions on finite-dimensional spaces. Some abstract convergence results for a broad class of linesearch methods are stablished. A general scheme for inexact reduced gradient (IRG) methods is proposed, where the errors in the gradient approximation automatically adapt with the magnitudes of the exact gradients. The sequences of iterations are shown to obtain stationary accumulation points when different stepsize selections are employed. Convergence results with constructive convergence rates for the developed IRG methods are established under the Kurdyka- Lojasiewicz property. The obtained results for the IRG methods are confirmed by encouraging numerical experiments, which demonstrate advantages of automatically controlled errors in IRG methods over other frequently used error selections.},
  author={Khanh, P. D. and Mordukhovich, B. S. and Tran, D. B.},
  journal={Journal of Optimization Theory and Applications},
  pages={1-41},
  year={2023},
  doi={10.1007/s10957-023-02319-9},
  publisher=Springer,
  html={https://link.springer.com/article/10.1007/s10957-023-02319-9},
  dimensions={true},
  pdf={IRG.pdf},
  google_scholar_id={UeHWp8X0CEIC},
  selected={true},
}

@article{kmtOMS,
  abbr={Journal},
  title={A new inexact gradient descent method with applications to nonsmooth convex optimization},
  author={PD Khanh, BS Mordukhovich, DB Tran},
  journal={Optimization Methods and Software},
  abstract={sadas},
  html={https://www.tandfonline.com/doi/full/10.1080/10556788.2024.2322700},
  pdf={example_pdf.pdf},
  pages={1-29},
  year={2024},
}

@article{kmptJOGOprox,
  abbr={Journal},
  bibtex_show={true},
  title={Inexact proximal methods for weakly convex functions},
  author={Khanh, P. D. and Mordukhovich, B. S. and Phat, V. T and Tran, D. B.},
  journal={to appear Journal of Global Optimization},
  url={https://arxiv.org/pdf/2307.15596},
  dimensions={true},
  year={2024}
}

@article{kmtMAPR,
  abbr={Preprint},
  bibtex_show={true},
  title={Globally Convergent Derivative-Free Methods in Nonconvex Optimization with and without Noise},
  author={PD Khanh, BS Mordukhovich, DB Tran},
  url={https://optimization-online.org/?p=26889},
  year={2024}
}
@article{ckmtMAPR,
  abbr={Preprint},
  bibtex_show={true},
  title={Local Convergence Analysis for Nonisolated Solutions to Derivative-Free Methods of Optimization},
  author={DH Cuong, PD Khanh, BS Mordukhovich, DB Tran},
  url={https://optimization-online.org/?p=27216},
  year={2024}
}